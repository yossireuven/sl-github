{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zD4fRb3dGRaO"
   },
   "source": [
    "# Excerise - Logistic Regression and Classification Measures\n",
    "In this exercise you will explore how hard it is to find the best balance between TPR and FPR. \n",
    "\n",
    "Next, you will implement a logistic regression classifier using gradient decent.\n",
    "\n",
    "We will then turn to applying multi-class classifiers over the well known MNIST digits database, and analyse the results using a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2p-7eGqNjrp"
   },
   "source": [
    "## Balancing TPR and FPR and interpretability of a classifier results given inbalance between classes\n",
    "After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive for a serious disease, and that the test is 99% accurate (i.e. the probability of testing positive given that you have the disease is 0.99, as is the probability of testing negative given that you donâ€™t have the disease). The good news is that this is a rare disease, striking only 1 in 10,000 people. \n",
    "\n",
    "1. What is the accuracy of always predicting a patient is healthy? Would that be a good strategy?\n",
    "\n",
    "2. What are the TPR and FPR of the chosen classifier in this case? Giving that there is a tradeoff between the two --- is it a good point on the ROC curve?\n",
    "\n",
    "3. Why is it good news that the disease is rare? What are the chances that you actually have the disease?\n",
    "\n",
    "4. Taking into account the previous answers, would you say the designer of the classifier made a good choice in this case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cluKn7Rb2LDB"
   },
   "source": [
    "#### Answer 1\n",
    "Answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTLwPg6q3TEg"
   },
   "source": [
    "#### Answer 2\n",
    "Answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mX39tM4K3UTB"
   },
   "source": [
    "#### Answer 3\n",
    "Answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tISo5Put6P-Q"
   },
   "source": [
    "#### Answer 4\n",
    "Answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJU3fUbuTzKm"
   },
   "source": [
    "## Implementing Logistic Regression Model\n",
    "Recall the loss function of the binary (Univariate) Logistic Regression model\n",
    "\n",
    "$$ J(\\beta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\left(y^{(i)}\\log(h_\\beta(x^{(i)}))+(1-y^{(i)})\\log(1-h_\\beta(x^{(i)}))\\right) $$\n",
    "\n",
    "and the gradient decent update rule is given by\n",
    "\n",
    "$$ \\beta_j^t := \\beta_j^{t-1}-\\mu\\frac{\\partial}{\\partial \\beta_j}J(\\beta)\n",
    "$$\n",
    "\n",
    "1. Write the explicit (and rather simple) update rule for a gradient decent solver of the Logistic Regression model.\n",
    "2. Implement a LogisticRegression class with `fit`, `predict`, and `score` functions, and the constructor of the class should have the following parameters (feel free to add methods and parameters as you see fit):\n",
    "  * `method`: either `GD` or `SGD`, indiacting whether to use a regular gradient decent, or an [iterative stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method) respectively.\n",
    "  * `learning_rate`: the learning rate to be used\n",
    "  * `num_iterations`: number of iterations for the gradient decent algorithms (**note** that for stochastic gradient decent, each iteration is interpreted in that aspect as an epoch of going over all the samples, as it is computationaly similar to a single iteration of regular gradient decent)\n",
    "  * `track_loss`: a boolean stating if to track the loss of the model for each iteration during training; if `True` the model will have a `loss` vector with the loss that was claculated for each of the iterations.\n",
    "3. Use the provided code to load the Iris dataset as a toy example, on which we will compare the results of the two logistic regression solvers. Run both solvers (GD and SGD) for 1000 iterations, using a learning rate of 0.1, and compare the learning curve by plotting the loss during training. Describe the results, and plot the decision boundary for each of the models. What would you see if you plot the loss after each parameters update (for each sample in every epoch)?\n",
    " \n",
    " \n",
    "For simplicity, do not use regularization, and we will not try to find the best learning rate --- however, you are more than welcome to play with those on your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQVMUZkQHlm2"
   },
   "source": [
    "## Classifying Digits\n",
    "In this part we will test digits classification on the MNIST dataset, using Multivariate Logistic Regression (a discriminative model). \n",
    "\n",
    "The MNIST dataset contains 28x28 grayscale images of handwritten digits between 0 and 9 (10 classes). For mathmatical analysis clarity, and for matching expected API, each image faltten to create a 1D array with 784 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cjwjk6pzLE-y"
   },
   "source": [
    "### Loading the MNIST dataset\n",
    "Load the MNIST data set. The digits dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. Use \n",
    ">```mnist = sklearn.datasets.fetch_mldata('MNIST original')```\n",
    "\n",
    "to fetch the original data. You may set the `data_home` to where you wish to download your data for caching. Each image is already transformed into a 1D integer array $x\\in [0,255]^{784}$, and the corresponding label is an integer $y\\in [0,9]$.\n",
    "\n",
    "Plot a single sample of each digit as the original image, so you get a feeling how the data looks like.\n",
    "\n",
    "Finally, divide your data into train and test sets, using 1/7 of the data for testing.\n",
    "\n",
    "---\n",
    "**Note:** Using `digits = sklearn.datasets.load_digits()` will only fetch a very small sample of the original set, with images resized to 8x8. This preprocessing of the data reduces dimensionality and gives invariance to small distortions - however, we will use the original data in this exercise. Feel free to test the proformance of the algorithms below on the preprocessed data as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05Om0QLxBvvT"
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Jv-ecdm6uTg"
   },
   "source": [
    "### Multivariate Logistic Regression\n",
    "We will now apply the Multivariate Logistic Regression model discriminative model for this problem. We will use a prepared implementation of the model, so you won't need to implement it.\n",
    "\n",
    "In this part we will keep the pixel values in grayscale, and assume the probability of each image being classified as one of the 10 digit classes, matches some multivariate logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PU3K28Yu6uTh"
   },
   "source": [
    "#### Question 1\n",
    "Run a Multivariate Logitic Regression classifier on the training data and apply predictions on the test data. Use the [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation, and set the `solver` to be `'lbfgs'`, and  `multi_class` to `'multinomial'` (leave all other parameters in their default value).\n",
    "\n",
    "1. Plot the confusion matrix of your classifier, as claculated on the test data (it is recommended to use [sklearn.metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)). Calculate the total accuracy (fraction of correctly classified images), and summarize the results in your own words.\n",
    "\n",
    "2. Show some test images which were missclassified (classified incorrectly). Choose one such sample, and output the probability estimates for all classes (use the `predict_proba` function). Describe the results.\n",
    "\n",
    "Note that here we cannot \"generate\" examples from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEWJXu5BclVn"
   },
   "source": [
    "#### Answer 1\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNRDBGgyofrT"
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNOTlRjZN9xW"
   },
   "source": [
    "#### Question 2\n",
    "Comparing two methods for solving multiclass classification problems: one-vs-rest (OvR), and multinomial.\n",
    "\n",
    "1. Discuss the results, and explain the plots in [Plot multinomial and One-vs-Rest Logistic Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html).\n",
    "\n",
    "2. Create another Logistic Regression classifier, setting now `multi_class` to `'ovr'` (instead of `multinomial`). Leave all other parameters the same. Compare the accuracy of this model with the previous one model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LspgymK47DMx"
   },
   "source": [
    "#### Answer 2\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOLKjDPk7DMy"
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression and Classification Measures - Exercise.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
