{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zD4fRb3dGRaO"
   },
   "source": [
    "# Excerise - Logistic Regression and Classification Measures\n",
    "In this exercise you will explore how hard it is to find the best balance between TPR and FPR. \n",
    "\n",
    "Next, you will implement a logistic regression classifier using gradient decent.\n",
    "\n",
    "We will then turn to applying multi-class classifiers over the well known MNIST digits database, and analyse the results using a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2p-7eGqNjrp"
   },
   "source": [
    "## Balancing TPR and FPR and interpretability of a classifier results given inbalance between classes\n",
    "After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive for a serious disease, and that the test is 99% accurate (i.e. the probability of testing positive given that you have the disease is 0.99, as is the probability of testing negative given that you donâ€™t have the disease). The good news is that this is a rare disease, striking only 1 in 10,000 people. \n",
    "\n",
    "1. What is the accuracy of always predicting a patient is healthy? Would that be a good strategy?\n",
    "\n",
    "2. What are the TPR and FPR of the chosen classifier in this case? Giving that there is a tradeoff between the two --- is it a good point on the ROC curve?\n",
    "\n",
    "3. Why is it good news that the disease is rare? What are the chances that you actually have the disease?\n",
    "\n",
    "4. Taking into account the previous answers, would you say the designer of the classifier made a good choice in this case?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cluKn7Rb2LDB"
   },
   "source": [
    "#### Answer 1\n",
    "Always predicting that a patient is healty mean that $TP=0$ and $FP=0$, given that we find that:\n",
    "\n",
    "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TN}{TN+FN} = 0.99$\n",
    "\n",
    "Predicting always healty can lead to good accuracy for the model, but in real life it will lead to  100 deads for every 1,000,000 people... SAD :-("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTLwPg6q3TEg"
   },
   "source": [
    "#### Answer 2\n",
    "\n",
    "$ TPR=\\frac{TP}{TP + FN}= 0.99$\n",
    "\n",
    "$ FPR=\\frac{FP}{TN + FP}= 0.01$\n",
    "\n",
    "Yes thats a good point on the ROC, because the TPR is high meaning we classify sick correctly and vice versa for FPR (low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mX39tM4K3UTB"
   },
   "source": [
    "#### Answer 3\n",
    "The chances that one actually have the disease is:\n",
    "\n",
    "$P(sick|positive) =(*) \\frac{P(positive|sick)*P(sick)}{P(positive)} =(**) \\frac{P(positive|sick)*P(sick)}{P(positive|sick)*P(sick) + P(positive|healty)*P(healty)} = 0.0098$\n",
    "\n",
    "$\\text{(*) Bayes' theorem},    \\text{(**) Law of total probability}$\n",
    "\n",
    "The good news is that even if one have tested positive the probability of him being sick is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tISo5Put6P-Q"
   },
   "source": [
    "#### Answer 4\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJU3fUbuTzKm"
   },
   "source": [
    "## Implementing Logistic Regression Model\n",
    "Recall the loss function of the binary (Univariate) Logistic Regression model\n",
    "\n",
    "$$ J(\\beta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\left(y^{(i)}\\log(h_\\beta(x^{(i)}))+(1-y^{(i)})\\log(1-h_\\beta(x^{(i)}))\\right) $$\n",
    "\n",
    "and the gradient decent update rule is given by\n",
    "\n",
    "$$ \\beta_j^t := \\beta_j^{t-1}-\\mu\\frac{\\partial}{\\partial \\beta_j}J(\\beta)\n",
    "$$\n",
    "\n",
    "1. Write the explicit (and rather simple) update rule for a gradient decent solver of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_j^t := \\beta_j^{t-1}-\\mu\\frac{\\partial}{\\partial \\beta_j}J(\\beta) = \\beta_j^{t-1}-\\mu\\sum_{i=1}^{m}(y^{(i)} - h_\\beta(x^{(i)}))x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJU3fUbuTzKm"
   },
   "source": [
    "2. Implement a LogisticRegression class with `fit`, `predict`, and `score` functions, and the constructor of the class should have the following parameters (feel free to add methods and parameters as you see fit):\n",
    "  * `method`: either `GD` or `SGD`, indiacting whether to use a regular gradient decent, or an [iterative stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method) respectively.\n",
    "  * `learning_rate`: the learning rate to be used\n",
    "  * `num_iterations`: number of iterations for the gradient decent algorithms (**note** that for stochastic gradient decent, each iteration is interpreted in that aspect as an epoch of going over all the samples, as it is computationaly similar to a single iteration of regular gradient decent)\n",
    "  * `track_loss`: a boolean stating if to track the loss of the model for each iteration during training; if `True` the model will have a `loss` vector with the loss that was claculated for each of the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, method='GD', learning_rate=0.1, track_loss=True, iters=1000):\n",
    "        self.method = method\n",
    "        self.lr = learning_rate\n",
    "        self.track = track_loss\n",
    "        self.iters = iters\n",
    "        self.w = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        classes = np.array(list(set(y)))\n",
    "        print('Found {} labels'.format(classes.size))\n",
    "        lb = LabelBinarizer()\n",
    "        lb.fit(y)\n",
    "        y = lb.transform(y)\n",
    "        # init weights\n",
    "        w = np.zeros((X.shape[1], 1)) \n",
    "        for i in range(self.iters):\n",
    "            w, cost = self.update_(X, y, w)\n",
    "            if i % 100 == 0:\n",
    "                print('iter: {}, cost: {}'.format(i, cost.sum()))\n",
    "\n",
    "#         self.w = np.zeros((X.shape[1], classes.size))  \n",
    "#         for c in np.arange(classes.size):\n",
    "#             w = np.zeros((X.shape[1], classes.size))\n",
    "#             for i in range(self.iters):\n",
    "#                 w, cost = self.update_(X, y, w)\n",
    "#                 if i % 100 == 0:\n",
    "#                     print('iter: {}, cost: {}'.format(i, cost.sum()))\n",
    "#             self.w = w\n",
    "            \n",
    "    def update_(self, X, y, w):\n",
    "        m = y.shape[0]\n",
    "        hx = self.sigmoid(X.dot(w))\n",
    "        J = -1*(1/m)*(np.log(hx).T.dot(y) + np.log(1-hx).T.dot(1-y))\n",
    "        #gradient = (1/m)*X.T.dot(hx - np.expand_dims(y, axis=1))\n",
    "        gradient = (1/m)*X.T.dot(hx - y)\n",
    "        w -= self.lr*gradient\n",
    "        return w, J\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(X.dot(self.w))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJU3fUbuTzKm"
   },
   "source": [
    "3. Use the provided code to load the Iris dataset as a toy example, on which we will compare the results of the two logistic regression solvers. Run both solvers (GD and SGD) for 1000 iterations, using a learning rate of 0.1, and compare the learning curve by plotting the loss during training. Describe the results, and plot the decision boundary for each of the models. What would you see if you plot the loss after each parameters update (for each sample in every epoch)?\n",
    " \n",
    " \n",
    "For simplicity, do not use regularization, and we will not try to find the best learning rate --- however, you are more than welcome to play with those on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X = np.c_[np.ones(X.shape[0]), X]    # bias padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 labels\n",
      "iter: 0, cost: 0.6931471805599452\n",
      "iter: 100, cost: 0.4372119701982426\n",
      "iter: 200, cost: 0.3326355359640812\n",
      "iter: 300, cost: 0.27778245442592214\n",
      "iter: 400, cost: 0.24411233387156053\n",
      "iter: 500, cost: 0.22125875407731235\n",
      "iter: 600, cost: 0.2046550945551776\n",
      "iter: 700, cost: 0.1919917748096203\n",
      "iter: 800, cost: 0.1819773714547795\n",
      "iter: 900, cost: 0.17383313409932125\n"
     ]
    }
   ],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(X[50:], y[50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It seem that i have error when trying MultiClass **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQVMUZkQHlm2"
   },
   "source": [
    "## Classifying Digits\n",
    "In this part we will test digits classification on the MNIST dataset, using Multivariate Logistic Regression (a discriminative model). \n",
    "\n",
    "The MNIST dataset contains 28x28 grayscale images of handwritten digits between 0 and 9 (10 classes). For mathmatical analysis clarity, and for matching expected API, each image faltten to create a 1D array with 784 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cjwjk6pzLE-y"
   },
   "source": [
    "### Loading the MNIST dataset\n",
    "Load the MNIST data set. The digits dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. Use \n",
    ">```mnist = sklearn.datasets.fetch_mldata('MNIST original')```\n",
    "\n",
    "to fetch the original data. You may set the `data_home` to where you wish to download your data for caching. Each image is already transformed into a 1D integer array $x\\in [0,255]^{784}$, and the corresponding label is an integer $y\\in [0,9]$.\n",
    "\n",
    "Plot a single sample of each digit as the original image, so you get a feeling how the data looks like.\n",
    "\n",
    "Finally, divide your data into train and test sets, using 1/7 of the data for testing.\n",
    "\n",
    "---\n",
    "**Note:** Using `digits = sklearn.datasets.load_digits()` will only fetch a very small sample of the original set, with images resized to 8x8. This preprocessing of the data reduces dimensionality and gives invariance to small distortions - however, we will use the original data in this exercise. Feel free to test the proformance of the algorithms below on the preprocessed data as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\217216X717997\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.base import get_data_home \n",
    "print (get_data_home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05Om0QLxBvvT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 500: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-385-df5ab1cd2041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch_mldata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST original'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\datasets\\mldata.py\u001b[0m in \u001b[0;36mfetch_mldata\u001b[1;34m(dataname, target_name, data_name, transpose_data, data_home)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0murlname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLDATA_BASE_URL\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mquote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mmldata_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 642\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\217216x717997\\appdata\\local\\programs\\python\\python36\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 500: Internal Server Error"
     ]
    }
   ],
   "source": [
    "mnist = datasets.fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Jv-ecdm6uTg"
   },
   "source": [
    "### Multivariate Logistic Regression\n",
    "We will now apply the Multivariate Logistic Regression model discriminative model for this problem. We will use a prepared implementation of the model, so you won't need to implement it.\n",
    "\n",
    "In this part we will keep the pixel values in grayscale, and assume the probability of each image being classified as one of the 10 digit classes, matches some multivariate logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PU3K28Yu6uTh"
   },
   "source": [
    "#### Question 1\n",
    "Run a Multivariate Logitic Regression classifier on the training data and apply predictions on the test data. Use the [sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation, and set the `solver` to be `'lbfgs'`, and  `multi_class` to `'multinomial'` (leave all other parameters in their default value).\n",
    "\n",
    "1. Plot the confusion matrix of your classifier, as claculated on the test data (it is recommended to use [sklearn.metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)). Calculate the total accuracy (fraction of correctly classified images), and summarize the results in your own words.\n",
    "\n",
    "2. Show some test images which were missclassified (classified incorrectly). Choose one such sample, and output the probability estimates for all classes (use the `predict_proba` function). Describe the results.\n",
    "\n",
    "Note that here we cannot \"generate\" examples from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEWJXu5BclVn"
   },
   "source": [
    "#### Answer 1\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNRDBGgyofrT"
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNOTlRjZN9xW"
   },
   "source": [
    "#### Question 2\n",
    "Comparing two methods for solving multiclass classification problems: one-vs-rest (OvR), and multinomial.\n",
    "\n",
    "1. Discuss the results, and explain the plots in [Plot multinomial and One-vs-Rest Logistic Regression](https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_multinomial.html).\n",
    "\n",
    "2. Create another Logistic Regression classifier, setting now `multi_class` to `'ovr'` (instead of `multinomial`). Leave all other parameters the same. Compare the accuracy of this model with the previous one model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LspgymK47DMx"
   },
   "source": [
    "#### Answer 2\n",
    "Put you answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JOLKjDPk7DMy"
   },
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regression and Classification Measures - Exercise.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
